# Awesome 3D Spatial Reasoning

A curated list of datasets, papers, and codebases on 3D spatial reasoning.

**Contribution:** If you would like to contribute to this list, please submit a [pull request](https://github.com/wufeim/awesome-3d-spatial-reasoning/pulls).

## Papers

> **Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2504.15280-b31b1b.svg)](https://arxiv.org/abs/2504.15280)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://danielchyeh.github.io/All-Angles-Bench/)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-All--Angles--Bench-FFD21E?logo=huggingface)](https://huggingface.co/datasets/ch-chenyu/All-Angles-Bench)
> [![GitHub](https://img.shields.io/badge/GitHub-All--Angles--Bench-000?logo=github&logoColor=fff)](https://github.com/Chenyu-Wang567/All-Angles-Bench)

> **Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2504.14151-b31b1b.svg)](https://arxiv.org/abs/2504.14151)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://ai.meta.com/research/publications/locate-3d-real-world-object-localization-via-self-supervised-learning-in-3d/)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-locate--3d-FFD21E?logo=huggingface)](https://huggingface.co/facebook/locate-3d)
> [![GitHub](https://img.shields.io/badge/GitHub-locate--3d-000?logo=github&logoColor=fff)](https://github.com/facebookresearch/locate-3d)

> **Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2504.01805-b31b1b.svg)](https://arxiv.org/abs/2504.01805)

> **Improved Visual-Spatial Reasoning via R1-Zero-Like Training**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2504.00883-b31b1b.svg)](https://arxiv.org/abs/2504.00883)

> **From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2503.22976-b31b1b.svg)](https://arxiv.org/abs/2503.22976)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://fudan-zvg.github.io/spar/)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-SPAR--7M-FFD21E?logo=huggingface)](https://huggingface.co/datasets/jasonzhango/SPAR-7M)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-SPAR--Bench-FFD21E?logo=huggingface)](https://huggingface.co/datasets/jasonzhango/SPAR-Bench)
> [![GitHub](https://img.shields.io/badge/GitHub-spar-000?logo=github&logoColor=fff)](https://github.com/fudan-zvg/spar)

> **Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2503.19707-b31b1b.svg)](https://arxiv.org/abs/2503.19707)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-srbench-FFD21E?logo=huggingface)](https://huggingface.co/datasets/stogiannidis/srbench)
> [![GitHub](https://img.shields.io/badge/GitHub-srbench-000?logo=github&logoColor=fff)](https://github.com/stogiannidis/srbench)

> **Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2503.11094-b31b1b.svg)](https://www.arxiv.org/abs/2503.11094)
> [![GitHub](https://img.shields.io/badge/GitHub-Open3DVQA-000?logo=github&logoColor=fff)](https://github.com/WeichenZh/Open3DVQA)

> **SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2502.13143-b31b1b.svg)](https://arxiv.org/abs/2502.13143)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://qizekun.github.io/sofar/)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-sofar-FFD21E?logo=huggingface)](https://huggingface.co/collections/qizekun/sofar-67b511129d3146d28cea9920)
> [![GitHub](https://img.shields.io/badge/GitHub-SoFar-000?logo=github&logoColor=fff)](https://github.com/qizekun/SoFar)

> **PulseCheck457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models**<br/>
> CVPR 2025 ⭐️ Highlight<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2502.08636-b31b1b.svg)](https://arxiv.org/abs/2502.08636)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-Spatial457-FFD21E?logo=huggingface)](https://huggingface.co/datasets/RyanWW/Spatial457)
> [![GitHub](https://img.shields.io/badge/GitHub-Spatial457-000?logo=github&logoColor=fff)](https://github.com/XingruiWang/Spatial457)

> **PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding**<br/>
> ICLR 2025<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2501.16411-b31b1b.svg)](https://arxiv.org/abs/2501.16411)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://physbench.github.io)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-PhysBench-FFD21E?logo=huggingface)](https://huggingface.co/datasets/USC-GVL/PhysBench)
> [![GitHub](https://img.shields.io/badge/GitHub-PhysBench-000?logo=github&logoColor=fff)](https://github.com/USC-GVL/PhysBench)

> **Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2412.14171-b31b1b.svg)](https://arxiv.org/abs/2412.14171)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://vision-x-nyu.github.io/thinking-in-space.github.io/)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-VSI--Bench-FFD21E?logo=huggingface)](https://huggingface.co/datasets/nyu-visionx/VSI-Bench)
> [![GitHub](https://img.shields.io/badge/GitHub-thinking--in--space-000?logo=github&logoColor=fff)](https://github.com/vision-x-nyu/thinking-in-space)

> **SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2412.07755-b31b1b.svg)](https://arxiv.org/abs/2412.07755)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://arijitray.com/SAT/)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-SAT-FFD21E?logo=huggingface)](https://huggingface.co/datasets/array/SAT)

> **3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2412.07825-b31b1b.svg)](https://arxiv.org/abs/2412.07825)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://3dsrbench.github.io)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-3DSRBench-FFD21E?logo=huggingface)](https://huggingface.co/datasets/ccvl/3DSRBench)

> **Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding**<br/>
> CVPR 2025<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2412.00493-b31b1b.svg)](https://arxiv.org/abs/2412.00493)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-Video--3D--LLM_data-FFD21E?logo=huggingface)](https://huggingface.co/datasets/zd11024/Video-3D-LLM_data)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-Video3D--LLM--LLaVA--Qwen--Uniform--32-FFD21E?logo=huggingface)](https://huggingface.co/zd11024/Video3D-LLM-LLaVA-Qwen-Uniform-32)
> [![GitHub](https://img.shields.io/badge/GitHub-Video--3D--LLM-000?logo=github&logoColor=fff)](https://github.com/LaVi-Lab/Video-3D-LLM)

> **Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities**<br/>
> ICLR 2025 ⭐️ Oral<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2410.17385-b31b1b.svg)](https://arxiv.org/abs/2410.17385)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://spatial-comfort.github.io)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-COMFORT-FFD21E?logo=huggingface)](https://huggingface.co/datasets/sled-umich/COMFORT)
> [![GitHub](https://img.shields.io/badge/GitHub-COMFORT-000?logo=github&logoColor=fff)](https://github.com/sled-group/COMFORT)

> **Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs**<br/>
> NeurIPS 2024 ⭐️ Oral<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2406.16860-b31b1b.svg)](https://arxiv.org/abs/2406.16860)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://cambrian-mllm.github.io)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-cambrian--data-FFD21E?logo=huggingface)](https://huggingface.co/collections/nyu-visionx/cambrian-data-6667ce801e179b4fbe774e11)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-cambrian--1--models-FFD21E?logo=huggingface)](https://huggingface.co/collections/nyu-visionx/cambrian-1-models-666fa7116d5420e514b0f23c)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-CV--Bench-FFD21E?logo=huggingface)](https://huggingface.co/datasets/nyu-visionx/CV-Bench)
> [![GitHub](https://img.shields.io/badge/GitHub-cambrian-000?logo=github&logoColor=fff)](https://github.com/cambrian-mllm/cambrian)

> **SpatialBot: Precise Spatial Understanding with Vision Language Models**<br/>
> ICRA 2025<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2406.13642-b31b1b.svg)](https://arxiv.org/abs/2406.13642)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-SpatialQA-FFD21E?logo=huggingface)](https://huggingface.co/datasets/RussRobin/SpatialQA)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-SpatialQA--E-FFD21E?logo=huggingface)](https://huggingface.co/datasets/RussRobin/SpatialQA-E)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-SpatialBench-FFD21E?logo=huggingface)](https://huggingface.co/datasets/RussRobin/SpatialBench)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-SpatialBot--3B-FFD21E?logo=huggingface)](https://huggingface.co/RussRobin/SpatialBot-3B)
> [![GitHub](https://img.shields.io/badge/GitHub-SpatialBot-000?logo=github&logoColor=fff)](https://github.com/BAAI-DCAI/SpatialBot)

> **ImageNet3D: Towards General-Purpose Object-Level 3D Understanding**<br/>
> NeurIPS 2024<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2406.09613-b31b1b.svg)](https://arxiv.org/abs/2406.09613)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://imagenet3d.github.io)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-ImageNet3D-FFD21E?logo=huggingface)](https://huggingface.co/datasets/ccvl/ImageNet3D)
> [![GitHub](https://img.shields.io/badge/GitHub-imagenet3d-000?logo=github&logoColor=fff)](https://github.com/wufeim/imagenet3d)
> [![GitHub](https://img.shields.io/badge/GitHub-imagenet3d_exp-000?logo=github&logoColor=fff)](https://github.com/wufeim/imagenet3d_exp)

> **SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models**<br/>
> NeurIPS 2024<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2406.01584-b31b1b.svg)](https://arxiv.org/abs/2406.01584)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://www.anjiecheng.me/SpatialRGPT)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-OpenSpatialDataset-FFD21E?logo=huggingface)](https://huggingface.co/datasets/a8cheng/OpenSpatialDataset)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-SpatialRGPT--Bench-FFD21E?logo=huggingface)](https://huggingface.co/datasets/a8cheng/SpatialRGPT-Bench)
> [![GitHub](https://img.shields.io/badge/GitHub-SpatialRGPT-000?logo=github&logoColor=fff)](https://github.com/AnjieCheng/SpatialRGPT)

> **Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering**<br/>
> ICLR 2025<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2406.00622-b31b1b.svg)](https://arxiv.org/abs/2406.00622)
> [![GitHub](https://img.shields.io/badge/GitHub-DynSuperCLEVR-000?logo=github&logoColor=fff)](https://github.com/XingruiWang/DynSuperCLEVR)

> **BLINK: Multimodal Large Language Models Can See but Not Perceive**<br/>
> ECCV 2024<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2404.12390-b31b1b.svg)](https://arxiv.org/abs/2404.12390)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://zeyofu.github.io/blink/)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-BLINK-FFD21E?logo=huggingface)](https://huggingface.co/datasets/BLINK-Benchmark/BLINK)
> [![GitHub](https://img.shields.io/badge/GitHub-BLINK_Benchmark-000?logo=github&logoColor=fff)](https://github.com/zeyofu/BLINK_Benchmark)

> **SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities**<br/>
> CVPR 2024<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2401.12168-b31b1b.svg)](https://arxiv.org/abs/2401.12168)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://spatial-vlm.github.io)

> **Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models**<br/>
> CVPR 2024<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2401.00988-b31b1b.svg)](https://arxiv.org/abs/2401.00988)
> [![GitHub](https://img.shields.io/badge/GitHub-NuInstruct-000?logo=github&logoColor=fff)](https://github.com/xmed-lab/NuInstruct)

> **3D-Aware Visual Question Answering about Parts, Poses and Occlusions**<br/>
> NeurIPS 2023<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2310.17914-b31b1b.svg)](https://arxiv.org/abs/2310.17914)
> [![GitHub](https://img.shields.io/badge/GitHub-3D--Aware--VQA-000?logo=github&logoColor=fff)](https://github.com/XingruiWang/3D-Aware-VQA)
> [![GitHub](https://img.shields.io/badge/GitHub-superclevr--3D--question-000?logo=github&logoColor=fff)](https://github.com/XingruiWang/superclevr-3D-question)
