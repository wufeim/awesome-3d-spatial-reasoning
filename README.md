# Awesome 3D Spatial Reasoning

A curated list of datasets, papers, and codebases on 3D spatial reasoning.

**Contribution:** If you would like to contribute to this list, please submit a [pull request](https://github.com/wufeim/awesome-3d-spatial-reasoning/pulls).

## Papers

> **Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2504.14151-b31b1b.svg)](https://arxiv.org/abs/2504.14151)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://ai.meta.com/research/publications/locate-3d-real-world-object-localization-via-self-supervised-learning-in-3d/)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-locate--3d-FFD21E?logo=huggingface)](https://huggingface.co/facebook/locate-3d)
> [![GitHub](https://img.shields.io/badge/GitHub-locate--3d-000?logo=github&logoColor=fff)](https://github.com/facebookresearch/locate-3d)

> **Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2504.01805-b31b1b.svg)](https://arxiv.org/abs/2504.01805)

> **Improved Visual-Spatial Reasoning via R1-Zero-Like Training**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2504.00883-b31b1b.svg)](https://arxiv.org/abs/2504.00883)

> **From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2503.22976-b31b1b.svg)](https://arxiv.org/abs/2503.22976)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://fudan-zvg.github.io/spar/)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-SPAR--7M-FFD21E?logo=huggingface)](https://huggingface.co/datasets/jasonzhango/SPAR-7M)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-SPAR--Bench-FFD21E?logo=huggingface)](https://huggingface.co/datasets/jasonzhango/SPAR-Bench)
> [![GitHub](https://img.shields.io/badge/GitHub-spar-000?logo=github&logoColor=fff)](https://github.com/fudan-zvg/spar)

> **Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2503.11094-b31b1b.svg)](https://www.arxiv.org/abs/2503.11094)
> [![GitHub](https://img.shields.io/badge/GitHub-Open3DVQA-000?logo=github&logoColor=fff)](https://github.com/WeichenZh/Open3DVQA)

> **SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2502.13143-b31b1b.svg)](https://arxiv.org/abs/2502.13143)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://qizekun.github.io/sofar/)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-sofar-FFD21E?logo=huggingface)](https://huggingface.co/collections/qizekun/sofar-67b511129d3146d28cea9920)
> [![GitHub](https://img.shields.io/badge/GitHub-SoFar-000?logo=github&logoColor=fff)](https://github.com/qizekun/SoFar)

> **PulseCheck457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models**<br/>
> CVPR 2025 ⭐️ highlight<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2502.08636-b31b1b.svg)](https://arxiv.org/abs/2502.08636)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-Spatial457-FFD21E?logo=huggingface)](https://huggingface.co/datasets/RyanWW/Spatial457)
> [![GitHub](https://img.shields.io/badge/GitHub-Spatial457-000?logo=github&logoColor=fff)](https://github.com/XingruiWang/Spatial457)

> **PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding**<br/>
> ICLR 2025<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2501.16411-b31b1b.svg)](https://arxiv.org/abs/2501.16411)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://physbench.github.io)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-PhysBench-FFD21E?logo=huggingface)](https://huggingface.co/datasets/USC-GVL/PhysBench)
> [![GitHub](https://img.shields.io/badge/GitHub-PhysBench-000?logo=github&logoColor=fff)](https://github.com/USC-GVL/PhysBench)

> **SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2412.07755-b31b1b.svg)](https://arxiv.org/abs/2412.07755)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://arijitray.com/SAT/)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-SAT-FFD21E?logo=huggingface)](https://huggingface.co/datasets/array/SAT)

> **3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark**<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2412.07825-b31b1b.svg)](https://arxiv.org/abs/2412.07825)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://3dsrbench.github.io)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-3DSRBench-FFD21E?logo=huggingface)](https://huggingface.co/datasets/ccvl/3DSRBench)

> **ImageNet3D: Towards General-Purpose Object-Level 3D Understanding**<br/>
> NeurIPS 2024<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2406.09613-b31b1b.svg)](https://arxiv.org/abs/2406.09613)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://imagenet3d.github.io)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-ImageNet3D-FFD21E?logo=huggingface)](https://huggingface.co/datasets/ccvl/ImageNet3D)
> [![GitHub](https://img.shields.io/badge/GitHub-imagenet3d-000?logo=github&logoColor=fff)](https://github.com/wufeim/imagenet3d)
> [![GitHub](https://img.shields.io/badge/GitHub-imagenet3d_exp-000?logo=github&logoColor=fff)](https://github.com/wufeim/imagenet3d_exp)

> **SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models**<br/>
> NeurIPS 2024<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2406.01584-b31b1b.svg)](https://arxiv.org/abs/2406.01584)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://www.anjiecheng.me/SpatialRGPT)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-OpenSpatialDataset-FFD21E?logo=huggingface)](https://huggingface.co/datasets/a8cheng/OpenSpatialDataset)
> [![HuggingFace](https://img.shields.io/badge/Hugging%20Face-SpatialRGPT--Bench-FFD21E?logo=huggingface)](https://huggingface.co/datasets/a8cheng/SpatialRGPT-Bench)
> [![GitHub](https://img.shields.io/badge/GitHub-SpatialRGPT-000?logo=github&logoColor=fff)](https://github.com/AnjieCheng/SpatialRGPT)

> **Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering**<br/>
> ICLR 2025<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2406.00622-b31b1b.svg)](https://arxiv.org/abs/2406.00622)
> [![GitHub](https://img.shields.io/badge/GitHub-DynSuperCLEVR-000?logo=github&logoColor=fff)](https://github.com/XingruiWang/DynSuperCLEVR)

> **SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities**<br/>
> CVPR 2024<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2401.12168-b31b1b.svg)](https://arxiv.org/abs/2401.12168)
> [![webpage](https://img.shields.io/badge/webpage-9cf)](https://spatial-vlm.github.io)

> **Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models**<br/>
> CVPR 2024<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2401.00988-b31b1b.svg)](https://arxiv.org/abs/2401.00988)
> [![GitHub](https://img.shields.io/badge/GitHub-NuInstruct-000?logo=github&logoColor=fff)](https://github.com/xmed-lab/NuInstruct)

> **3D-Aware Visual Question Answering about Parts, Poses and Occlusions**<br/>
> NeurIPS 2023<br/>
> [![arXiv](https://img.shields.io/badge/arXiv-2310.17914-b31b1b.svg)](https://arxiv.org/abs/2310.17914)
> [![GitHub](https://img.shields.io/badge/GitHub-3D--Aware--VQA-000?logo=github&logoColor=fff)](https://github.com/XingruiWang/3D-Aware-VQA)
> [![GitHub](https://img.shields.io/badge/GitHub-superclevr--3D--question-000?logo=github&logoColor=fff)](https://github.com/XingruiWang/superclevr-3D-question)
